{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5959ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/spark/work-dir\n"
     ]
    }
   ],
   "source": [
    "!pwd  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64221a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data  random_forest.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfc6ff7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.4\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(pyspark.__version__)\n",
    "# import py4j\n",
    "# !pip install cassandra-driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f80b052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "from cassandra.cluster import Cluster\n",
    "import pandas as pd\n",
    "from cassandra.auth import PlainTextAuthProvider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4fd100",
   "metadata": {},
   "source": [
    "### 1. Chargement des données brutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b114fac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: string (nullable = true)\n",
      " |-- sexe: string (nullable = true)\n",
      " |-- taux: string (nullable = true)\n",
      " |-- situationFamiliale: string (nullable = true)\n",
      " |-- nbEnfantsAcharge: string (nullable = true)\n",
      " |-- 2eme voiture: string (nullable = true)\n",
      " |-- immatriculation: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- marque: string (nullable = true)\n",
      " |-- nom: string (nullable = true)\n",
      " |-- puissance: integer (nullable = true)\n",
      " |-- longueur: string (nullable = true)\n",
      " |-- nbPlaces: integer (nullable = true)\n",
      " |-- nbPortes: integer (nullable = true)\n",
      " |-- couleur: string (nullable = true)\n",
      " |-- occasion: boolean (nullable = true)\n",
      " |-- prix: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- immatriculation: string (nullable = true)\n",
      " |-- marque: string (nullable = true)\n",
      " |-- nom: string (nullable = true)\n",
      " |-- puissance: integer (nullable = true)\n",
      " |-- longueur: string (nullable = true)\n",
      " |-- nbPlaces: integer (nullable = true)\n",
      " |-- nbPortes: integer (nullable = true)\n",
      " |-- couleur: string (nullable = true)\n",
      " |-- occasion: boolean (nullable = true)\n",
      " |-- prix: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialiser Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Data Cleaning with Spark\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Charger les données brutes\n",
    "clients = spark.read.csv(\"data/raw/clients.csv\", header=True, inferSchema=True)\n",
    "catalogue = spark.read.csv(\"data/raw/catalogue.csv\", header=True, inferSchema=True)\n",
    "immatriculation = spark.read.csv(\"data/raw/immatriculations.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Afficher les schémas\n",
    "clients.printSchema()\n",
    "catalogue.printSchema()\n",
    "immatriculation.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ec616da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(age='25', sexe='F', taux='159', situationFamiliale='En Couple', nbEnfantsAcharge='2', 2eme voiture='false', immatriculation='3467 SB 72')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clients.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f6832c",
   "metadata": {},
   "source": [
    "### 2. Exploration des données (EDA) et nettoyage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30743afb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7c6f06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se connecter à Cassandra\n",
    "auth_provider = PlainTextAuthProvider(username='cassandra', password='cassandra')\n",
    "cluster = Cluster(['172.18.0.2'], port=9042, auth_provider=auth_provider)\n",
    "session = cluster.connect('concessionnaire')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46c61c4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidRequest",
     "evalue": "Error from server: code=2200 [Invalid query] message=\"Invalid STRING constant (25) for \"age\" of type int\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequest\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m         session\u001b[38;5;241m.\u001b[39mexecute(query, \u001b[38;5;28mtuple\u001b[39m(row))\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Insérer les données\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43minsert_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclients\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclients\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# insert_data(session, \"catalogue\", catalogue)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# insert_data(session, \"immatriculation\", immatriculation)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDonnées insérées avec succès dans Cassandra.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 13\u001b[0m, in \u001b[0;36minsert_data\u001b[0;34m(session, table, data)\u001b[0m\n\u001b[1;32m     11\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(row))\n\u001b[1;32m     12\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINSERT INTO \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) VALUES (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalues\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 13\u001b[0m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/cassandra/cluster.py:2677\u001b[0m, in \u001b[0;36mcassandra.cluster.Session.execute\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/cassandra/cluster.py:4956\u001b[0m, in \u001b[0;36mcassandra.cluster.ResponseFuture.result\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mInvalidRequest\u001b[0m: Error from server: code=2200 [Invalid query] message=\"Invalid STRING constant (25) for \"age\" of type int\""
     ]
    }
   ],
   "source": [
    "# Charger les données depuis les fichiers CSV\n",
    "clients = pd.read_csv(\"/opt/spark/work-dir/data/raw/clients.csv\")\n",
    "catalogue = pd.read_csv(\"/opt/spark/work-dir/data/raw/catalogue.csv\")\n",
    "immatriculation = pd.read_csv(\"/opt/spark/work-dir/data/raw/immatriculations.csv\")\n",
    "\n",
    "# Insérer les données dans Cassandra\n",
    "def insert_data(session, table, data):\n",
    "    for index, row in data.iterrows():\n",
    "        # Encadrer les noms de colonnes avec des espaces\n",
    "        columns = ', '.join([f'\"{col}\"' if ' ' in col else col for col in row.index])\n",
    "        values = ', '.join(['%s'] * len(row))\n",
    "        query = f\"INSERT INTO {table} ({columns}) VALUES ({values})\"\n",
    "        session.execute(query, tuple(row))\n",
    "\n",
    "# Insérer les données\n",
    "insert_data(session, \"clients\", clients)\n",
    "# insert_data(session, \"catalogue\", catalogue)\n",
    "# insert_data(session, \"immatriculation\", immatriculation)\n",
    "\n",
    "print(\"Données insérées avec succès dans Cassandra.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60db6c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session with Cassandra connector\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RandomForestWithCassandra\") \\\n",
    "    .config(\"spark.cassandra.connection.host\", \"172.18.0.2\") \\\n",
    "    .config(\"spark.cassandra.connection.port\", \"9042\") \\\n",
    "    .config(\"spark.cassandra.auth.username\", \"cassandra\") \\\n",
    "    .config(\"spark.cassandra.auth.password\", \"cassandra\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.datastax.spark:spark-cassandra-connector_2.12:3.2.0\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22826660",
   "metadata": {},
   "source": [
    "### Script to load data into Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8df47d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:cassandra.cluster:Cluster.__init__ called with contact_points specified, but no load_balancing_policy. In the next major version, this will raise an error; please specify a load-balancing policy. (contact_points = ['172.18.0.2'], lbp = None)\n",
      "WARNING:cassandra.cluster:Downgrading core protocol version from 66 to 65 for 172.18.0.2:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
      "WARNING:cassandra.cluster:Downgrading core protocol version from 65 to 5 for 172.18.0.2:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
      "WARNING:cassandra.connection:An authentication challenge was not sent, this is suspicious because the driver expects authentication (configured authenticator = PlainTextAuthenticator)\n",
      "WARNING:cassandra.connection:An authentication challenge was not sent, this is suspicious because the driver expects authentication (configured authenticator = PlainTextAuthenticator)\n"
     ]
    }
   ],
   "source": [
    "# Se connecter à Cassandra\n",
    "# Configuration de la connexion Cassandra\n",
    "auth_provider = PlainTextAuthProvider(username='cassandra', password='cassandra')  \n",
    "cluster = Cluster(['172.18.0.2'], port=9042, auth_provider=auth_provider)  \n",
    "session = cluster.connect('concessionnaire')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dfc276bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidRequest",
     "evalue": "Error from server: code=2200 [Invalid query] message=\"Invalid STRING constant (25) for \"age\" of type int\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequest\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m clients \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/raw/clients.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m clients\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m----> 4\u001b[0m     \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;250;43m        \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;43;03m        INSERT INTO clients (immatriculation, age, sexe, taux, situationFamiliale, nbEnfantsAcharge, \"2eme voiture\")\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;43;03m        VALUES (%s, %s, %s, %s, %s, %s, %s)\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;43;03m        \"\"\"\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimmatriculation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msexe\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtaux\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msituationFamiliale\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnbEnfantsAcharge\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2eme voiture\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/cassandra/cluster.py:2677\u001b[0m, in \u001b[0;36mcassandra.cluster.Session.execute\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/cassandra/cluster.py:4956\u001b[0m, in \u001b[0;36mcassandra.cluster.ResponseFuture.result\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mInvalidRequest\u001b[0m: Error from server: code=2200 [Invalid query] message=\"Invalid STRING constant (25) for \"age\" of type int\""
     ]
    }
   ],
   "source": [
    "# Load Clients Data\n",
    "clients = pd.read_csv(\"./data/raw/clients.csv\")\n",
    "for _, row in clients.iterrows():\n",
    "    session.execute(\n",
    "        \"\"\"\n",
    "        INSERT INTO clients (immatriculation, age, sexe, taux, situationFamiliale, nbEnfantsAcharge, \"2eme voiture\")\n",
    "        VALUES (%s, %s, %s, %s, %s, %s, %s)\n",
    "        \"\"\",\n",
    "        (row['immatriculation'], row['age'], row['sexe'], row['taux'], row['situationFamiliale'], row['nbEnfantsAcharge'], row['2eme voiture'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33757d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load Catalogue Data\n",
    "catalogue = pd.read_csv(\"../data/processed/catalogue_processed.csv\")\n",
    "for _, row in catalogue.iterrows():\n",
    "    session.execute(\n",
    "        \"\"\"\n",
    "        INSERT INTO catalogue (marque, nom, puissance, longueur, nbPlaces, nbPortes, prix, occasion)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        \"\"\",\n",
    "        (row['marque'], row['nom'], row['puissance'], row['longueur'], row['nbPlaces'], row['nbPortes'], row['prix'], row['occasion'])\n",
    "    )\n",
    "\n",
    "# Load Immatriculations Data\n",
    "immatriculations = pd.read_csv(\"../data/processed/immatriculations_processed.csv\")\n",
    "for _, row in immatriculations.iterrows():\n",
    "    session.execute(\n",
    "        \"\"\"\n",
    "        INSERT INTO immatriculations (immatriculation, marque, nom, puissance, longueur, nbPlaces, nbPortes, prix, occasion)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        \"\"\",\n",
    "        (row['immatriculation'], row['marque'], row['nom'], row['puissance'], row['longueur'], row['nbPlaces'], row['nbPortes'], row['prix'], row['occasion'])\n",
    "    )\n",
    "\n",
    "print(\"Data loaded into Cassandra successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b61ddb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- immatriculation: string (nullable = false)\n",
      " |-- 2eme voiture: boolean (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- nbenfantsacharge: integer (nullable = true)\n",
      " |-- sexe: string (nullable = true)\n",
      " |-- situationfamiliale: string (nullable = true)\n",
      " |-- taux: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/17 20:55:34 WARN V2ScanPartitioningAndOrdering: Spark ignores the partitioning CassandraPartitioning. Please use KeyGroupedPartitioning for better performance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+---+----------------+----+------------------+----+\n",
      "|immatriculation|2eme voiture|age|nbenfantsacharge|sexe|situationfamiliale|taux|\n",
      "+---------------+------------+---+----------------+----+------------------+----+\n",
      "+---------------+------------+---+----------------+----+------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data from Cassandra (exemple avec la table `clients`)\n",
    "clients = spark.read \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .options(table=\"clients\", keyspace=\"concessionnaire\") \\\n",
    "    .load()\n",
    "\n",
    "# Show data schema and preview\n",
    "clients.printSchema()\n",
    "clients.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39f032c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- marque: string (nullable = false)\n",
      " |-- nom: string (nullable = true)\n",
      " |-- couleur: string (nullable = true)\n",
      " |-- longueur: string (nullable = true)\n",
      " |-- nbplaces: integer (nullable = true)\n",
      " |-- nbportes: integer (nullable = true)\n",
      " |-- occasion: boolean (nullable = true)\n",
      " |-- prix: decimal(38,18) (nullable = true)\n",
      " |-- puissance: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/17 20:55:54 WARN V2ScanPartitioningAndOrdering: Spark ignores the partitioning CassandraPartitioning. Please use KeyGroupedPartitioning for better performance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+-------+--------+--------+--------+--------+----+---------+\n",
      "|marque|nom|couleur|longueur|nbplaces|nbportes|occasion|prix|puissance|\n",
      "+------+---+-------+--------+--------+--------+--------+----+---------+\n",
      "+------+---+-------+--------+--------+--------+--------+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data from Cassandra (exemple avec la table `catalogue`)\n",
    "catalogue = spark.read \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .options(table=\"catalogue\", keyspace=\"concessionnaire\") \\\n",
    "    .load()\n",
    "\n",
    "# Show data schema and preview\n",
    "catalogue.printSchema()\n",
    "catalogue.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "042fbdb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- immatriculation: string (nullable = false)\n",
      " |-- couleur: string (nullable = true)\n",
      " |-- longueur: string (nullable = true)\n",
      " |-- marque: string (nullable = true)\n",
      " |-- nbplaces: integer (nullable = true)\n",
      " |-- nbportes: integer (nullable = true)\n",
      " |-- nom: string (nullable = true)\n",
      " |-- occasion: boolean (nullable = true)\n",
      " |-- prix: decimal(38,18) (nullable = true)\n",
      " |-- puissance: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/17 20:56:55 WARN V2ScanPartitioningAndOrdering: Spark ignores the partitioning CassandraPartitioning. Please use KeyGroupedPartitioning for better performance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+--------+------+--------+--------+---+--------+----+---------+\n",
      "|immatriculation|couleur|longueur|marque|nbplaces|nbportes|nom|occasion|prix|puissance|\n",
      "+---------------+-------+--------+------+--------+--------+---+--------+----+---------+\n",
      "+---------------+-------+--------+------+--------+--------+---+--------+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data from Cassandra (exemple avec la table `immatriculation`)\n",
    "immatriculation = spark.read \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .options(table=\"immatriculation\", keyspace=\"concessionnaire\") \\\n",
    "    .load()\n",
    "\n",
    "# Show data schema and preview\n",
    "immatriculation.printSchema()\n",
    "immatriculation.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15d579c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare data\n",
    "assembler = VectorAssembler(inputCols=[\"feature1\", \"feature2\", \"feature3\"], outputCol=\"features\")\n",
    "data = assembler.transform(df).select(\"features\", \"label\")\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb94eecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/14 21:47:49 WARN V2ScanPartitioningAndOrdering: Spark ignores the partitioning CassandraPartitioning. Please use KeyGroupedPartitioning for better performance\n",
      "25/01/14 21:47:50 WARN V2ScanPartitioningAndOrdering: Spark ignores the partitioning CassandraPartitioning. Please use KeyGroupedPartitioning for better performance\n",
      "25/01/14 21:47:51 WARN V2ScanPartitioningAndOrdering: Spark ignores the partitioning CassandraPartitioning. Please use KeyGroupedPartitioning for better performance\n",
      "25/01/14 21:47:52 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 2 (= number of training instances)\n",
      "25/01/14 21:47:54 WARN V2ScanPartitioningAndOrdering: Spark ignores the partitioning CassandraPartitioning. Please use KeyGroupedPartitioning for better performance\n",
      "25/01/14 21:47:54 WARN V2ScanPartitioningAndOrdering: Spark ignores the partitioning CassandraPartitioning. Please use KeyGroupedPartitioning for better performance\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train Random Forest model\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10)\n",
    "model = rf.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74ab95c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/14 21:47:58 WARN V2ScanPartitioningAndOrdering: Spark ignores the partitioning CassandraPartitioning. Please use KeyGroupedPartitioning for better performance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: nan\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate the model\n",
    "predictions = model.transform(test_data)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Model Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11ed877e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Stop Spark session\n",
    "# spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f9bf04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
